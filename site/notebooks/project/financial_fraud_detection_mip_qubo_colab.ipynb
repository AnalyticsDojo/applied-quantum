{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "\n# Financial Fraud Detection \u2014 Classical + QUBO on One Dataset (Colab-Ready)\n\nThis notebook builds a **fraud scoring** model and an **alert selection** policy using a single synthetic dataset:\n\n1) **Classical path**\n   - Train a **logistic regression** (implemented from scratch) to predict fraud probability \\( \\hat{p}_i \\) per transaction.\n   - Optimize which alerts to **review** via a **MILP** (PuLP): choose up to a review **capacity** \\(B\\) to maximize expected utility. Includes a **greedy fallback** if PuLP isn't available.\n   - Visualize ROC & Precision\u2013Recall.\n\n2) **QUBO path (quantum-ready)**\n   - Build a **transaction affinity graph** (shared device/merchant) to capture ring behavior.\n   - Select alerts with a **QUBO** that combines: expected utility, a **budget** penalty, and a **pairwise coupling** that promotes selecting **connected suspicious clusters**. Solve with `neal` (simulated annealing).\n\nWe reuse the same dataset and compare the review sets and utilities.\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\n# Install dependencies if needed\ndef _silent_imports():\n    flags = {\"pulp\": False, \"dimod\": False, \"neal\": False}\n    try:\n        import pulp\n        flags[\"pulp\"] = True\n    except Exception:\n        pass\n    try:\n        import dimod\n        flags[\"dimod\"] = True\n    except Exception:\n        pass\n    try:\n        import neal\n        flags[\"neal\"] = True\n    except Exception:\n        pass\n    return flags\n\nflags = _silent_imports()\nif not flags[\"pulp\"]:\n    %pip -q install pulp\nif not flags[\"dimod\"] or not flags[\"neal\"]:\n    %pip -q install dimod neal\n\nflags = _silent_imports()\nprint(\"PuLP:\", flags[\"pulp\"], \"| dimod:\", flags[\"dimod\"], \"| neal:\", flags[\"neal\"])\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\n# ==== One Synthetic Dataset ====\nimport numpy as np, pandas as pd\n\nrng = np.random.default_rng(404)\n\nN = 3000                           # transactions\nreview_capacity = 250              # number of cases reviewers can handle\nreward_detect = 500.0              # benefit if a fraud is correctly reviewed/caught\ncost_false_pos = 25.0              # review cost + customer friction when it's not fraud\n\n# Features: amount, hour, merchant risk, device age, velocity (txn per hour last day), prior_declines\namount = np.exp(rng.normal(3.4, 0.9, size=N))  # log-normal-ish\nhour = rng.integers(0, 24, size=N)\nmerchant_risk = rng.uniform(0, 1, size=N)\ndevice_age_days = rng.integers(0, 400, size=N)\nvelocity = rng.exponential(scale=2.0, size=N)\nprior_declines = rng.binomial(4, 0.2, size=N)\n\n# Latent group effects (rings): shared device_id and merchant_id clusters\nn_devices = 450\nn_merchants = 220\ndevice_id = rng.integers(0, n_devices, size=N)\nmerchant_id = rng.integers(0, n_merchants, size=N)\n\n# True fraud logit model with group bumps\nw = np.array([0.0025,  # amount\n              0.05,    # hour (late-night risk)\n              1.2,     # merchant risk\n             -0.004,   # device age (older -> safer)\n              0.35,    # velocity\n              0.28])   # prior declines\nX = np.column_stack([amount, (hour>=22)|(hour<=5), merchant_risk, device_age_days, velocity, prior_declines]).astype(float)\n\n# Group risk terms\ndev_risk = rng.normal(0, 0.4, size=n_devices)\nmer_risk = rng.normal(0, 0.5, size=n_merchants)\ng_bump = dev_risk[device_id] + mer_risk[merchant_id]\n\nlogit = X @ w + 0.6*g_bump - 4.8  # base rate low\np_true = 1/(1+np.exp(-logit))\ny = rng.binomial(1, p_true)  # labels\n\ndf = pd.DataFrame({\n    \"amount\": amount, \"is_late\": ((hour>=22)|(hour<=5)).astype(int),\n    \"merchant_risk\": merchant_risk, \"device_age\": device_age_days,\n    \"velocity\": velocity, \"prior_declines\": prior_declines,\n    \"device_id\": device_id, \"merchant_id\": merchant_id,\n    \"label\": y\n})\nprint(\"Fraud rate:\", df[\"label\"].mean().round(4), \"| N:\", N)\ndf.head()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Part 1 \u2014 Classical: Manual Logistic Regression + MILP Alert Selection\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\n# Train/test split\nimport numpy as np, pandas as pd\n\nidx = np.arange(len(df))\nrng = np.random.default_rng(1)\nrng.shuffle(idx)\nsplit = int(0.75*len(df))\ntrain_idx, test_idx = idx[:split], idx[split:]\n\nfeatures = [\"amount\",\"is_late\",\"merchant_risk\",\"device_age\",\"velocity\",\"prior_declines\"]\nX_all = df[features].values.astype(float)\ny_all = df[\"label\"].values.astype(int)\n\nX_train, y_train = X_all[train_idx], y_all[train_idx]\nX_test, y_test   = X_all[test_idx],  y_all[test_idx]\n\n# Feature standardization (important for gradient)\nmu = X_train.mean(axis=0); sigma = X_train.std(axis=0) + 1e-9\nXtr = (X_train - mu)/sigma\nXte = (X_test  - mu)/sigma\n\n# Manual logistic regression (L2-regularized) via gradient descent\ndef train_logreg(X, y, lr=0.1, l2=1e-2, iters=800):\n    n, d = X.shape\n    w = np.zeros(d); b = 0.0\n    for t in range(iters):\n        z = X@w + b\n        p = 1/(1+np.exp(-z))\n        grad_w = X.T@(p - y)/n + l2*w\n        grad_b = (p - y).mean()\n        w -= lr*grad_w\n        b -= lr*grad_b\n    return w, b\n\nw_hat, b_hat = train_logreg(Xtr, y_train, lr=0.2, l2=5e-3, iters=1000)\n\ndef predict_proba(X):\n    z = X@w_hat + b_hat\n    return 1/(1+np.exp(-z))\n\np_train = predict_proba(Xtr)\np_test = predict_proba(Xte)\n\nprint(\"Train mean p\u0302:\", float(p_train.mean()).__round__(4), \"| Test mean p\u0302:\", float(p_test.mean()).__round__(4))\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\n# ROC and Precision\u2013Recall\nimport numpy as np, matplotlib.pyplot as plt\n\ndef roc_pr(y, p):\n    # thresholds from sorted unique probabilities\n    order = np.argsort(-p)\n    y_sorted = y[order]; p_sorted = p[order]\n    tps = np.cumsum(y_sorted==1)\n    fps = np.cumsum(y_sorted==0)\n    T = len(y)\n    P = (y==1).sum()\n    Nn = (y==0).sum()\n    # ROC\n    TPR = tps / max(P,1)\n    FPR = fps / max(Nn,1)\n    # PR\n    precision = tps / np.maximum((np.arange(T)+1), 1)\n    recall = TPR\n    return FPR, TPR, precision, recall\n\nFPR, TPR, Pprec, Rrec = roc_pr(y_test, p_test)\n\nplt.figure()\nplt.plot(FPR, TPR)\nplt.title(\"ROC (Test)\")\nplt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.tight_layout()\n\nplt.figure()\nplt.plot(Rrec, Pprec)\nplt.title(\"Precision\u2013Recall (Test)\")\nplt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.tight_layout()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "\n### MILP Alert Selection (capacity \\(B\\))\n\nWe decide which transactions to **review** to maximize **expected utility**:\n\\[\n\\max \\sum_i z_i \\big( \\hat{p}_i \\cdot R - (1-\\hat{p}_i)\\cdot C \\big)\n\\quad \\text{s.t. } \\sum_i z_i \\le B,\\; z_i\\in\\{0,1\\}.\n\\]\nIf PuLP is unavailable, we use a **greedy density** fallback (sort by \\( (\\hat{p}R - (1-\\hat{p})C) \\)).\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nimport numpy as np, pandas as pd\n\ntry:\n    import pulp\n    HAVE_PULP = True\nexcept Exception:\n    HAVE_PULP = False\n\n# We'll optimize on the **test** set (as if in production)\nutil = p_test*reward_detect - (1-p_test)*cost_false_pos\n\ndef solve_milp_alerts(util, capacity):\n    n = len(util)\n    prob = pulp.LpProblem(\"AlertSelection\", pulp.LpMaximize)\n    z = [pulp.LpVariable(f\"z_{i}\", lowBound=0, upBound=1, cat=\"Binary\") for i in range(n)]\n    prob += pulp.lpSum(float(util[i]) * z[i] for i in range(n))\n    prob += pulp.lpSum(z) <= int(capacity)\n    _ = prob.solve(pulp.PULP_CBC_CMD(msg=False))\n    status = pulp.LpStatus[prob.status]\n    z_sol = np.array([int(round(pulp.value(z[i]) or 0)) for i in range(n)])\n    obj = float(pulp.value(prob.objective))\n    return status, z_sol, obj\n\ndef greedy_alerts(util, capacity):\n    order = np.argsort(-util)\n    z = np.zeros(len(util), dtype=int)\n    z[order[:int(capacity)]] = 1\n    return \"Heuristic\", z, float(util[order[:int(capacity)]].sum())\n\nif HAVE_PULP:\n    status_mip, z_mip, obj_mip = solve_milp_alerts(util, review_capacity)\nelse:\n    status_mip, z_mip, obj_mip = greedy_alerts(util, review_capacity)\n\nprint(\"Alert selection (classical) \u2014 status:\", status_mip, \"| expected utility:\", round(obj_mip,2))\n"}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Part 2 \u2014 QUBO: Graph-Aware Alert Selection with Budget\n\nWe add **pairwise structure** to encourage selecting **connected suspicious rings**.\n\n- Binary \\(s_i\\): 1 if we **review** transaction \\(i\\).  \n- Linear benefit: \\(-U_i s_i\\) where \\(U_i = \\hat{p}_i R - (1-\\hat{p}_i)C\\) (negated for minimization).  \n- **Budget penalty:** \\(\\lambda (\\sum_i s_i - B)^2\\).  \n- **Graph coupling:** for edges \\( (i,j) \\) (shared device/merchant), add **negative** weight \\(-\\gamma w_{ij} s_i s_j\\) so selecting both lowers energy (promotes clusters).\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\nfrom collections import defaultdict\nimport numpy as np, pandas as pd\nimport dimod, neal\n\n# We'll build the QUBO on the **test set** for comparability\ndf_test = df.iloc[test_idx].copy()\nU = util  # expected utility vector for test set\nn = len(U)\n\n# Build a simple affinity graph (shared device or merchant)\n# weight 1.0 for shared device, 0.7 for shared merchant\nedges = []\nrow_index = np.arange(n)\n# Map device_id and merchant_id to indices within test set\ndev_to_rows = {}\nmer_to_rows = {}\nfor local_row, global_row in enumerate(test_idx):\n    dev_to_rows.setdefault(int(df.loc[global_row,\"device_id\"]), []).append(local_row)\n    mer_to_rows.setdefault(int(df.loc[global_row,\"merchant_id\"]), []).append(local_row)\n\nfor rows in dev_to_rows.values():\n    if len(rows) > 1:\n        for a_idx in range(len(rows)):\n            for b_idx in range(a_idx+1, len(rows)):\n                edges.append((rows[a_idx], rows[b_idx], 1.0))\n\nfor rows in mer_to_rows.values():\n    if len(rows) > 1:\n        for a_idx in range(len(rows)):\n            for b_idx in range(a_idx+1, len(rows)):\n                edges.append((rows[a_idx], rows[b_idx], 0.7))\n\n# QUBO parameters\nlam = float(5.0 * np.maximum(1.0, np.std(U)))   # budget penalty weight\ngamma = 1.5                                      # cluster coupling\n\nQ = defaultdict(float)\n\n# Linear utility (negated for minimization): -U_i\nfor i in range(n):\n    Q[(i,i)] += -float(U[i])\n\n# Budget penalty: lam*(sum s - B)^2\nB = float(review_capacity)\nfor i in range(n):\n    Q[(i,i)] += lam*(1 - 2*B)\nfor i in range(n):\n    for j in range(i+1, n):\n        Q[(i,j)] += 2*lam\n\n# Graph coupling: -gamma * w_ij * s_i s_j\nfor (i,j,w) in edges:\n    if i == j: continue\n    if i > j: i,j = j,i\n    Q[(i,j)] += -gamma * float(w)\n\n# Solve with SA\nbqm = dimod.BinaryQuadraticModel.from_qubo(dict(Q))\nsampleset = neal.SimulatedAnnealingSampler().sample(bqm, num_reads=1500)\nbest = sampleset.first\ns_qubo = np.array([best.sample.get(i,0) for i in range(n)], dtype=int)\n\nbudget_used = int(s_qubo.sum())\nutil_qubo = float((U * s_qubo).sum())\nprint(\"QUBO \u2014 selected:\", budget_used, \" (target:\", int(B), \") | expected utility:\", round(util_qubo,2))\n"}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": "\n# Compare classical vs QUBO selections\nimport numpy as np, pandas as pd\n\nchosen_classical = np.where(z_mip==1)[0]\nchosen_qubo = np.where(s_qubo==1)[0]\n\noverlap = len(set(chosen_classical).intersection(set(chosen_qubo)))\nprint(\"Classical selected:\", len(chosen_classical), \"| QUBO selected:\", len(chosen_qubo), \"| Overlap:\", overlap)\n\ndf_compare = pd.DataFrame({\n    \"idx\": np.arange(len(U)),\n    \"p_hat\": p_test,\n    \"utility\": U,\n    \"classical\": z_mip,\n    \"qubo\": s_qubo\n}).sort_values(\"p_hat\", ascending=False).head(15)\ndf_compare.head(15)\n"}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Wrap-up\n\n- The **classical** pipeline learns a probability model and uses **MILP** to pick the best alerts under capacity.  \n- The **QUBO** adds **structure** via a transaction graph, encouraging clusters typical of fraud rings, while also honoring a **soft** budget.  \n- Try varying `review_capacity`, `reward_detect`, `cost_false_pos`, and the QUBO weights `lam`, `gamma` to see trade-offs between **precision**, **recall**, and **cluster capture**.\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "nbformat": 4, "nbformat_minor": 5}